{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"nav_menu":{"height":"309px","width":"468px"},"toc":{"navigate_menu":true,"number_sections":true,"sideBar":true,"threshold":6,"toc_cell":false,"toc_section_display":"block","toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"E32UBMT7VKMm"},"source":["## Prepare python environment\n"]},{"cell_type":"code","metadata":{"id":"y_lm7Q-9VKMn"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ryOZJYQa3PG0"},"source":["random_state = 5 # use this to control randomness across runs e.g., dataset partitioning"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BASGXrOy4wat"},"source":["## Preparing the Glass Dataset (2 points)\n","\n","We will use glass dataset from UCI machine learning repository. Details for this data can be found [here](https://archive.ics.uci.edu/ml/datasets/glass+identification). The objective of the dataset is to identify the class of glass based on the following features:\n","\n","1.  RI: refractive index\n","2.  Na: Sodium\n","3.  Mg: Magnesium\n","4.  Al: Aluminum\n","5.  Si: Silica\n","6.  K: Potassium\n","7.  Ca: Calcium\n","8.  Ba: Barium\n","9.  Fe: Iron\n","10. Type of glass (Target label)\n","\n","The classes of glass are:\n","\n","1. building_windows_float_processed\n","2. building_windows_non_float_processed\n","3. vehicle_windows_float_processed\n","4. containers\n","6. tableware\n","7. headlamps\n","\n","Identification of glass from its content can be used for forensic analysis."]},{"cell_type":"markdown","metadata":{"id":"URgO9HCN6RCl"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"id":"tlKBDyEQDzrk"},"source":["# Download and load the dataset\n","import os\n","if not os.path.exists('glass.csv'):\n","    !wget \"https://github.com/jha-lab/ece364_2024/blob/main/data/glass.csv?raw=true\" -O glass.csv\n","df = pd.read_csv('glass.csv')\n","# Display the first five instances in the dataset\n","df.head(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Additional features to be added to the data\n","df['Ca_Na'] = df.Ca*df.Na\n","df['Al_Mg'] = df.Al*df.Mg\n","df['Ca_Mg'] = df.Ca*df.Mg\n","df['Ca_RI'] = df.Ca*df.RI"],"metadata":{"id":"U1ds9iRBPJwr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FRhEcln77rIK"},"source":["### Extract target and descriptive features (0.5 points)"]},{"cell_type":"code","metadata":{"id":"blhp_Upk8E-Y"},"source":["# Store all the features from the data in X\n","X = # insert your code here\n","# Store all the labels in y\n","y = # insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gdUFK3qG8Gnk"},"source":["# Convert data to numpy array\n","X = # insert your code here\n","y = # insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-JPYSnc8JQi"},"source":["### Create training and validation datasets (0.5 points)\n","\n","\n","Split the data into training and validation sets using `train_test_split`.  See [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for details. To get consistent result while splitting, set `random_state` to the value defined earlier. We use 80% of the data for training and 20% of the data for validation."]},{"cell_type":"code","metadata":{"id":"BzTzI3iT8R5x"},"source":["X_train, X_val, y_train, y_val = # insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hc40XakM8ZjC"},"source":["### Preprocess the dataset (1 point)\n","\n","#### Preprocess the data by normalizing each feature to have zero mean and unit standard deviation. This can be done using the `StandardScaler()` function. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details.\n"]},{"cell_type":"code","metadata":{"id":"aBcVWz4M8qi3"},"source":["# Define the scaler for scaling the data\n","scaler = # insert your code here\n","\n","# Normalize the training data\n","X_train = # insert your code here\n","\n","# Use the scaler defined above to standardize the validation data by applying the same transformation to the validation data.\n","X_val = # insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0ytKO6K6rW-"},"source":["## Training error-based models (18 points)\n","\n","\n","#### We will use the `sklearn` library to train a Multinomial Logistic Regression classifier and Support Vector Machines.\n"]},{"cell_type":"markdown","metadata":{"id":"hBAcnfZw6rW_"},"source":["### Exercise 1:  Learning a Multinomial Logistic Regression classifier (4 points)"]},{"cell_type":"markdown","metadata":{"id":"sZY5Qfz36rW_"},"source":["#### Use `sklearn`'s `SGDClassifier` to train a multinomial logistic regression classifier (i.e., using a one-versus-rest scheme) with Stochastic Gradient Descent. Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier) for more details.\n","\n","#### Set the `random_state` as defined above,  increase the `n_iter_no_change` to 100 and `max_iter` to 1000 to facilitate better convergence.  \n","\n","#### Report the model's accuracy over the training and validation sets.\n"]},{"cell_type":"code","metadata":{"id":"JJ3SYc4J6rW_"},"source":["from sklearn.linear_model import SGDClassifier\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1hqKVGd6rW_"},"source":["# insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cV7PfP9G6rXA"},"source":["#### Explain any performance difference observed between the training and validation datasets."]},{"cell_type":"markdown","metadata":{"id":"r080wHsO6rXA"},"source":["**ANS**:"]},{"cell_type":"markdown","metadata":{"id":"IctolF7v6rXA"},"source":["### Exercise 2: Learning a Support Vector Machine (SVM) (14 points)"]},{"cell_type":"markdown","metadata":{"id":"7uWiMMUs6rXA"},"source":["#### Use `sklearn`'s `SVC` class to train an SVM (i.e., using a [one-versus-one scheme](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-one)). Review ch.7 and see [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) for more details.\n"]},{"cell_type":"code","metadata":{"id":"Jyp0Jj3x6rXA"},"source":["from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U5dxvcSK6rXA"},"source":["#### Exercise 2a: Warm up (2 points)\n","\n","#### Train an SVM with a linear kernel. Set the  random_state to the value defined above. Keep all other parameters at their defaults.\n","\n","#### Report the model's accuracy over the training and validation sets."]},{"cell_type":"code","metadata":{"id":"REdlLnyO6rXB"},"source":["# insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dJELdAX36rXB"},"source":["#### Exercise 2b: Evaluate a polynomial kernel function (4 points)\n","\n","#### Try fitting an SVM with a polynomial kernel function and vary the degree among {1, 2, 3, 4}. Note that degree=1 yields a linear kernel.\n","\n","#### For each fitted classifier, report its accuracy over the training and validation sets.\n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=3`.  When the data is not linearly separable, this encourages the model to fit the training data. Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"B6T71FR_6rXB"},"source":["# insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdzaOyzg6rXB"},"source":["#### Explain the effect of increasing the degree of the polynomial."]},{"cell_type":"markdown","metadata":{"id":"vBeZEsy_6rXB"},"source":["**ANS**:"]},{"cell_type":"markdown","metadata":{"id":"GYfgKZuZ6rXB"},"source":["#### Exercise 2c: Evaluate the radial basis kernel function (6 points)\n","\n","#### Try fitting an SVM with a radial basis kernel function and vary the length-scale parameter given by $\\gamma$ among {0.01, 0.1,1,10, 100}.\n","\n","#### For each fitted classifier, report its accuracy over the training and validation sets.\n","\n","#### As before, set the random_state to the value defined above. Set the regularization strength `C=10`.  When the data is not linearly separable, this encourages the model to fit the training data (read more [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)). Keep all other parameters at their default values."]},{"cell_type":"code","metadata":{"id":"p3BCoYu66rXC"},"source":["# insert your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpzq9Ync6rXC"},"source":["#### Comment on the effect of increasing/reducing the length-scale parameter $\\gamma$. Also, compare the performance of the classifiers trained with RBF kernel function against those trained with the polynomial and linear kernel functions (i.e., Ex. 2b)."]},{"cell_type":"markdown","metadata":{"id":"M6_JKd8S6rXC"},"source":["**ANS**:"]},{"cell_type":"markdown","metadata":{"id":"rhxJEJpS6rXC"},"source":["#### Exercise 2d: Briefly state the main difference between the logistic regression classifier and the SVM. (2 points)"]},{"cell_type":"markdown","metadata":{"id":"1jPMmTUQ6rXD"},"source":["**ANS**:"]}]}