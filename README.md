# ECE 364: Machine Learning for Predictive Data Analytics

[![License](https://img.shields.io/badge/License-BSD%203--Clause-red.svg)](https://github.com/JHA-Lab/ece364_2024/main/LICENSE)
![Python 3.7, 3.8](https://img.shields.io/badge/python-3.11-blue.svg)
[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2Fjha-lab%2Fece364_2024&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
Welcome to ECE 364! In this course we present basic concepts in machine learning for predictive data analytics:
* **Information-based learning:** decision trees; Shannon’s entropy model; information gain; ID3 algorithm; feature selection; predicting continuous targets; tree pruning; model ensembles: boosting, bagging.
* **Similarity-based learning:** feature spac, distance metrics, nearest neighbor algorithm, noisy data, predicting continuous targets, various measures of similarity, feature selection.
* **Probability-based learning:** Bayes' theorem; Bayesian prediction; curse of dimensionality; conditional independence and factorization; Naive Bayes model; smoothing; continuous features: probability density functions, binning; Bayesian networks. 
* **Error-based learning:**  simple linear regression; error surface; multivariate linear regression; gradient descent; learning rate; handling categorical features; modeling nonlinear relationships; multinomial logistic regression; support vector machines.
* **Deep learning:** artificial neural networks; activation functions; backpropagation and gradient descent; vanishing gradients; weight initialization; categorical target features: softmax layer and cross-entropy loss; dropout.
* **Evaluation:** misclassification rate; confusion matrix: precision, recall, F1 measure, profit/loss; prediction scores: receiver operating characteristic curve, Kolmogorov-Smirnov statistic, gain/lift; measures of error; evaluating models after deployment.
* **The art of machine learning:** perspectives on prediction models: parametric vs. nonparametric, generative vs. discriminative; choosing a machine learning approach: no free lunch theorem, project requirements, data considerations.



## Course staff

* Instructor: [Niraj K. Jha](https://www.princeton.edu/~jha/)
* TAs: Hongjie Wang, Yihao Liang, Jiaxin Xiao

## Timings

* Lectures: M/W 3:00-4:20pm (EQuad B205)
* Office hours:
    * Niraj K. Jha: M/W 2-3pm (EQuad B205)
    * Hongjie Wang:
    * Yihao Liang: Tu: 1-2pm, Th:11am-12pm (EQuad B321)
    * Jiaxin Xiao:

Zoom links and TA contact information can be found on [Canvas](https://canvas.princeton.edu/).

 ## Grading

 * Assignments (25%)
    * A0: no grade
    * A1-A5: written theory portion (40 pts), Google Colab programming portion (20 pts)
    * A6: written theory portion (40 pts)
 * Mid-term exam (25%)
 * Final exam (50%)

## Reading

* [John D. Kelleher, Brian Mac Namee, and Aoife D’Arcy, Fundamentals of Machine Learning for Predictive Data Analytics, 2nd edition, The MIT Press](https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics-second-edition)

## Assignment Requirements

Each assignment in this course comprises two distinct parts: a theoretical segment, which you will complete using LaTeX, and a programming segment that can be done in either Google Colab or Anaconda.

Please prepare all theoretical submissions in LaTeX. You can find both the LaTeX template and a helpful tutorial in the `/docs` directory at `/docs/template.tex` and `/docs/Working with LaTeX.pdf`, respectively.

If you are unable to run the programming code on your local machine, [Google Colab](https://colab.research.google.com/) is an excellent alternative. Detailed instructions on how to use Google Colab can be found at `/docs/Working with Google Colab.pdf`.

More details could be found in `/docs/Assignment Requirements.pdf`.

## License

BSD-3-Clause. 
Copyright (c) 2023, JHA-Lab.
All rights reserved.

See License file for more details.
